{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(1)\n",
    "b = tf.constant(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = tf.add(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=3>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-17bb7203622b>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
      "Collecting sentencepiece!=0.1.92 (from transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/fa/2f/efa2492dda74720eef27bf1f2f210a2aa456d5d167cf5bcfee593ea54091/sentencepiece-0.1.91-cp36-cp36m-win_amd64.whl (1.2MB)\n",
      "Collecting tokenizers==0.8.1.rc1 (from transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/78/a8/dbb57717e7ddc5dc045fe13c403fb13bf64f4ecca9ea16c739d932071983/tokenizers-0.8.1rc1-cp36-cp36m-win_amd64.whl (1.9MB)\n",
      "Requirement already satisfied: requests in d:\\program\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers) (2.22.0)\n",
      "Collecting sacremoses (from transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/0e/05/157985b564418d06aa8f80f25f74d34a6b565eea714afc2bf5c595c17f25/regex-2020.7.14-cp36-cp36m-win_amd64.whl (268kB)\n",
      "Requirement already satisfied: numpy in d:\\program\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers) (1.18.1)\n",
      "Collecting dataclasses; python_version < \"3.7\" (from transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/e1/d2/6f02df2616fd4016075f60157c7a0452b38d8f7938ae94343911e0fb0b09/dataclasses-0.7-py3-none-any.whl\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\program\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers) (4.46.0)\n",
      "Requirement already satisfied: packaging in d:\\program\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers) (19.0)\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/93/83/71a2ee6158bb9f39a90c0dea1637f81d5eef866e188e1971a1b1ab01a35a/filelock-3.0.12-py3-none-any.whl\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\program\\anaconda3\\envs\\ml\\lib\\site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in d:\\program\\anaconda3\\envs\\ml\\lib\\site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\program\\anaconda3\\envs\\ml\\lib\\site-packages (from requests->transformers) (1.25.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in d:\\program\\anaconda3\\envs\\ml\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: six in d:\\program\\anaconda3\\envs\\ml\\lib\\site-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: click in d:\\program\\anaconda3\\envs\\ml\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in d:\\program\\anaconda3\\envs\\ml\\lib\\site-packages (from sacremoses->transformers) (0.13.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in d:\\program\\anaconda3\\envs\\ml\\lib\\site-packages (from packaging->transformers) (2.4.0)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py): started\n",
      "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\kongdezhi\\AppData\\Local\\pip\\Cache\\wheels\\29\\3c\\fd\\7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sentencepiece, tokenizers, regex, sacremoses, dataclasses, filelock, transformers\n",
      "Successfully installed dataclasses-0.7 filelock-3.0.12 regex-2020.7.14 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|███████████████████████████████████████████████████████████████████| 213k/213k [00:00<00:00, 255kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 122, 1695, 2532, 1545, 1559, 1604, 5603, 102], [101, 1367, 2532, 102, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['1 23 45678 78', '12 45'], padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test:\n",
    "    def __init__(self):\n",
    "        self.name = ['kong']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.name)\n",
    "    \n",
    "    def add(self, name):\n",
    "        self.name.append(name)\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        if i > 3:\n",
    "            raise StopIteration\n",
    "        return 'a', 'b', 'c'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kong']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.add('de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kong', 'de']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'b', 'c'), ('a', 'b', 'c'), ('a', 'b', 'c'), ('a', 'b', 'c')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████████████████████████████████████████████████████████████| 232k/232k [00:02<00:00, 97.8kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['88', '##7', '102', '##1', '88', '##6', '118', '##2', '149', '##1', '1016', '140', '##7', '102', '##0']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer_text = tokenizer.tokenize('887 1021 886 1182 1491 1016 1407 1020')\n",
    "print(tokenizer_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.decoders import WordPiece as WordPieceDecoder\n",
    "from tokenizers.models import WordPiece\n",
    "# from tokenizers.normalizers import Lowercase, NFKC, Sequence\n",
    "from tokenizers.pre_tokenizers import WhitespaceSplit\n",
    "\n",
    "# First we create an empty Byte-Pair Encoding model (i.e. not trained model)\n",
    "tokenizer = Tokenizer(WordPiece())\n",
    "\n",
    "# Then we enable lower-casing and unicode-normalization\n",
    "# The Sequence normalizer allows us to combine multiple Normalizer that will be\n",
    "# executed in order.\n",
    "# tokenizer.normalizer = Sequence([\n",
    "#     NFKC(),\n",
    "#     Lowercase()\n",
    "# ])\n",
    "\n",
    "# Our tokenizer also needs a pre-tokenizer responsible for converting the input to a ByteLevel representation.\n",
    "tokenizer.pre_tokenizer = WhitespaceSplit()\n",
    "\n",
    "# And finally, let's plug a decoder so we can recover from a tokenized input to the original one\n",
    "tokenizer.decoder = WordPieceDecoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained vocab size: 2671\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "# We initialize our trainer, giving him the details about the vocabulary we want to generate\n",
    "trainer = WordPieceTrainer(vocab_size=3770, show_progress=True)\n",
    "tokenizer.train(trainer, [\"E:/CodeSleepEatRepeat/data/58tech/samplesdataset/pre_train_data\"])\n",
    "\n",
    "print(\"Trained vocab size: {}\".format(tokenizer.get_vocab_size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded string: ['0', '5', '1222', '##3', '##4', '##5', '384']\n",
      "Decoded string: 0 5 1222345 384\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"0 5 1222345 384\")\n",
    "\n",
    "print(\"Encoded string: {}\".format(encoding.tokens))\n",
    "\n",
    "decoded = tokenizer.decode(encoding.ids)\n",
    "print(\"Decoded string: {}\".format(decoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2651\n"
     ]
    }
   ],
   "source": [
    "dic = {}\n",
    "with open('E:/CodeSleepEatRepeat/data/58tech/samplesdataset/pre_train_data', 'r') as f:\n",
    "    for line in f:\n",
    "        words = line.split(' ')\n",
    "        for w in words:\n",
    "            dic[w.strip()] = 1\n",
    "print(len(dic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2 0 0]\n",
      " [3 4 5 6]\n",
      " [6 7 0 0]], shape=(3, 4), dtype=int32)\n",
      "tf.Tensor([[8 0 0 0]], shape=(1, 4), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#padded_batch:构建批次，类似batch, 但可以填充到相同的形状。\n",
    "\n",
    "elements = [[1, 2],[3, 4, 5, 6],[6, 7],[8]]\n",
    "ds = tf.data.Dataset.from_generator(lambda: iter(elements), tf.int32)\n",
    "\n",
    "ds_padded_batch = ds.padded_batch(3,padded_shapes = [4,])\n",
    "for x in ds_padded_batch:\n",
    "    print(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['887', '1021', '886', '1182', '1491', '1016', '1407', '1020']\n",
      "['39', '17', '42', '1484', '147', '60', '63', '4', '139', '44']\n",
      "['305', '347', '34', '290', '289', '36', '34', '249', '291', '57', '368', '361', '237', '12', '64', '51', '40', '280']\n",
      "['0', '58', '54', '84', '188', '12', '142', '46', '580', '188', '42', '58', '54', '256', '2', '277', '305', '79']\n",
      "['18', '51', '40', '834', '79', '59', '60', '328', '123', '364', '23', '174', '526', '294', '4']\n"
     ]
    }
   ],
   "source": [
    "# 从文本文件构建数据管道\n",
    "\n",
    "ds5 = tf.data.TextLineDataset(\n",
    "    filenames = '../../data/58tech/samplesdataset/pre_train_data'\n",
    "    )\n",
    "\n",
    "for line in ds5.take(5):\n",
    "#     print(line.numpy().decode('utf-8'))\n",
    "    print(line.numpy().decode().split()[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "if 3 < 4:\n",
    "    print(3)\n",
    "elif 2 < 4:\n",
    "    print(2)\n",
    "else:\n",
    "    print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[1] = [0.1, 0.2, 0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 0. , 0. ],\n",
       "       [0.1, 0.2, 0.3]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[2.  1.1]\n",
      "  [3.1 4.2]]\n",
      "\n",
      " [[2.  1.1]\n",
      "  [3.1 4.2]]\n",
      "\n",
      " [[2.  1.1]\n",
      "  [3.1 4.2]]\n",
      "\n",
      " [[2.  1.1]\n",
      "  [3.1 4.2]]], shape=(4, 2, 2), dtype=float32)\n",
      "*****\n",
      "tf.Tensor(\n",
      "[[[2.  1.1]\n",
      "  [3.1 4.2]]\n",
      "\n",
      " [[2.  1.1]\n",
      "  [3.1 4.2]]\n",
      "\n",
      " [[2.  1.1]\n",
      "  [3.1 4.2]]\n",
      "\n",
      " [[2.  1.1]\n",
      "  [3.1 4.2]]], shape=(4, 2, 2), dtype=float32)\n",
      "*****\n",
      "tf.Tensor(\n",
      "[[[2.  1.1]\n",
      "  [3.1 4.2]]\n",
      "\n",
      " [[2.  1.1]\n",
      "  [3.1 4.2]]], shape=(2, 2, 2), dtype=float32)\n",
      "*****\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    for i in range(10):\n",
    "#         yield {'x':np.zeros([i,i]), 'y':np.zeros([i+1,i+1]), 'z':np.zeros([i+2,i+2])}\n",
    "        yield [[2.0,1.1],[3.1,4.2]], np.zeros([3,3]), np.zeros([4,4])\n",
    "\n",
    "elements = [[1, 2],[3, 4, 5, 6],[6, 7],[8]]\n",
    "ds = tf.data.Dataset.from_generator(lambda: test(), (tf.float32, tf.int32, tf.int32))\n",
    "\n",
    "ds_batch = ds.batch(4)\n",
    "for x in ds_batch:\n",
    "    print(x[0])\n",
    "    print('*'*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abc def']\n"
     ]
    }
   ],
   "source": [
    "x = 'abc def'\n",
    "xx = x.split('\\t')\n",
    "print(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit ('ML': conda)",
   "language": "python",
   "name": "python36864bitmlconda22b001fe9e6340f59031359cb65c132c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
